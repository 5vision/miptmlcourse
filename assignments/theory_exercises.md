
## Part 1

1. Derive LS formula for linear regression in matrix form.
2. Derive LS formula with L2 regularization.
3. Derive formula for gradient of binary cross entropy in logistic regression.
4. Show that establishing Gaussian prior on parameters is equivalent to L2 regularization.
5. Prove formulas for matrix calculus: gradient of linear and quadratic functions.
6. Prove that KL divergence is non-negative.
7. Compare a) parameter estimation through minimization of KL divergence between empirical distribution and true distribution and b) MLE estimator.
8. Derive MLE of parameters of Naive Bayes Classifier.
9. Prove that Hessian matrix for loss function in logistic regression is positive semi-definite.
10. Derive formulas for optimization step in Newton's algorithm.

## Part 2

1. Show that minimizing cross-entropy equivalent to minimizing KL divergence.
